<!-- Generated by SpecStory -->

# Describe experiments using mathematical terms (2025-06-11 18:18:17)

_**User**_

Please describe the experiments in the source in mathematical terms using Latex. 

---

_**Assistant**_

I'll analyze the experiments in the source code and describe them in mathematical terms using LaTeX.

---

Read file: true_episodic_memory.py

---

Read file: true_episodic_memory.py

---

Based on my analysis of the source code, I'll describe the experiments in mathematical terms using LaTeX.

## Mathematical Description of the Episodic Memory Experiments

### 1. TrueEpisodicMemory Model

The episodic memory system is defined by the following mathematical components:

**Memory Structure:**
- Memory keys: \( \mathbf{K} \in \mathbb{R}^{M \times d} \)
- Memory values: \( \mathbf{V} \in \mathbb{R}^{M \times d} \)  
- Memory contexts: \( \mathbf{C} \in \mathbb{R}^{M \times d_c} \)
- Memory timestamps: \( \mathbf{T} \in \mathbb{R}^{M \times 1} \)

where \( M \) is memory size, \( d \) is feature dimension, and \( d_c \) is context dimension.

**Retrieval Function:**

Given a query \( \mathbf{q} \in \mathbb{R}^d \) and optional context \( \mathbf{c} \in \mathbb{R}^{d_c} \), the retrieval process computes:

1. **Content-based similarity:**
   \[
   S_{\text{content}}(\mathbf{q}, \mathbf{K}) = \mathbf{q} \mathbf{K}^T
   \]

2. **Context-based similarity (if context provided):**
   \[
   S_{\text{context}}(\mathbf{c}, \mathbf{C}) = \mathbf{c} \mathbf{C}^T
   \]

3. **Temporal recency bias:**
   \[
   S_{\text{temporal}}(t) = \exp(-\lambda (t_{\text{current}} - \mathbf{T}))
   \]
   where \( \lambda = 0.1 \) is the time decay weight.

4. **Combined attention scores:**
   \[
   S_{\text{total}} = S_{\text{content}} + 0.5 \cdot S_{\text{context}} + 0.3 \cdot S_{\text{temporal}}
   \]

5. **Attention weights and retrieval:**
   \[
   \boldsymbol{\alpha} = \text{softmax}(S_{\text{total}} \odot \mathbf{m})
   \]
   \[
   \mathbf{r} = \boldsymbol{\alpha} \mathbf{V}
   \]
   where \( \mathbf{m} \) is the used slots mask and \( \mathbf{r} \) is the retrieved value.

**Learning-based Storage:**

For each episode \( (\mathbf{k}_i, \mathbf{v}_i, \mathbf{c}_i) \), the storage process minimizes:
\[
\mathcal{L} = \|\text{retrieve}(\mathbf{k}_i, \mathbf{c}_i) - \mathbf{v}_i\|_2^2
\]
through gradient descent updates over multiple steps.

### 2. Experiment 1: Temporal Interference

**Experimental Design:**

**Phase 1 - Morning Episodes:**
- Store episodes: \( \{(\mathbf{k}_i^{(m)}, \mathbf{v}_i^{(m)}, \mathbf{c}_i^{(m)})\}_{i=1}^5 \)
- Measure immediate recall accuracy: 
  \[
  A_{\text{immediate}} = \frac{1}{5} \sum_{i=1}^5 \cos(\text{retrieve}(\mathbf{k}_i^{(m)}, \mathbf{c}_i^{(m)}), \mathbf{v}_i^{(m)})
  \]

**Phase 2 - Interfering Afternoon Episodes:**
- Generate interfering keys: \( \mathbf{k}_j^{(a)} = \mathbf{k}_{j \bmod 3}^{(m)} + 0.2 \boldsymbol{\epsilon}_j \)
- Store 15 interfering episodes with different contexts and values

**Phase 3 - Delayed Recall:**
- Measure delayed recall accuracy:
  \[
  A_{\text{delayed}} = \frac{1}{5} \sum_{i=1}^5 \cos(\text{retrieve}(\mathbf{k}_i^{(m)}, \mathbf{c}_i^{(m)}), \mathbf{v}_i^{(m)})
  \]

**Interference Effect:**
\[
I = A_{\text{immediate}} - A_{\text{delayed}}
\]

**Context Selectivity:**
For the same key \( \mathbf{k}_1^{(m)} \):
\[
\text{Selectivity} = |\cos(\text{retrieve}(\mathbf{k}_1^{(m)}, \mathbf{c}_1^{(m)}), \mathbf{v}_1^{(m)}) - \cos(\text{retrieve}(\mathbf{k}_1^{(m)}, \mathbf{c}_1^{(a)}), \mathbf{v}_1^{(a)})|
\]

### 3. Experiment 2: Sequential Episodic Recall

**Sequence Generation:**
Create a coherent sequence with gradual drift:
\[
\mathbf{k}_i = \mathbf{k}_{\text{base}} + 0.1 \cdot i \cdot \boldsymbol{\epsilon}_i
\]
\[
\mathbf{v}_i = \mathbf{v}_{\text{base}} + 0.1 \cdot i \cdot \boldsymbol{\delta}_i
\]
where \( \boldsymbol{\epsilon}_i, \boldsymbol{\delta}_i \sim \mathcal{N}(0, \mathbf{I}) \) and \( i \in \{0, 1, \ldots, L-1\} \) for sequence length \( L = 6 \).

**Sequential vs Random Recall:**

**Sequential accuracy:**
\[
A_{\text{seq}} = \frac{1}{L} \sum_{i=0}^{L-1} \cos(\text{retrieve}(\mathbf{k}_i, \mathbf{c}), \mathbf{v}_i)
\]

**Random order accuracy:**
\[
A_{\text{rand}} = \frac{1}{L} \sum_{j=0}^{L-1} \cos(\text{retrieve}(\mathbf{k}_{\pi(j)}, \mathbf{c}), \mathbf{v}_{\pi(j)})
\]
where \( \pi \) is a random permutation of \( \{0, 1, \ldots, L-1\} \).

**Sequential advantage:**
\[
\Delta A = A_{\text{seq}} - A_{\text{rand}}
\]

### Key Mathematical Properties

1. **Temporal Decay:** The exponential decay function \( \exp(-\lambda \Delta t) \) models forgetting over time.

2. **Multi-modal Attention:** The combination of content, context, and temporal similarities creates a rich retrieval mechanism.

3. **Interference Modeling:** Similar keys \( \mathbf{k}_j^{(a)} \approx \mathbf{k}_i^{(m)} \) create interference through overlapping attention patterns.

4. **Learning-based Storage:** The MSE loss \( \|\mathbf{r} - \mathbf{v}_{\text{target}}\|_2^2 \) ensures that retrieval matches intended storage.

These experiments demonstrate how episodic memory systems can exhibit temporal interference effects and context-dependent retrieval, similar to human episodic memory phenomena.

---

_**User**_

Great! Could you add to this the mathematical description of the forward function?

---

_**Assistant**_

I'll add a detailed mathematical description of the forward function to complement the existing analysis.

## Mathematical Description of the Forward Function

The forward function implements a multi-modal attention mechanism that combines content-based, contextual, and temporal retrieval cues. Here's the complete mathematical formulation:

### Forward Function: \( \text{retrieve}(\mathbf{q}, \mathbf{c}, \lambda) \rightarrow \mathbf{r} \)

**Input Parameters:**
- Query vector: \( \mathbf{q} \in \mathbb{R}^{b \times d} \) (batch size \( b \), dimension \( d \))
- Context vector: \( \mathbf{c} \in \mathbb{R}^{b \times d_c} \) (optional, context dimension \( d_c \))
- Time weight: \( \lambda \in \mathbb{R}^+ \) (default: 0.1)

**Step 1: Content-Based Attention Scores**
\[
S_{\text{content}} = \mathbf{q} \mathbf{K}^T \in \mathbb{R}^{b \times M}
\]
where \( \mathbf{K} \in \mathbb{R}^{M \times d} \) are the memory keys and \( M \) is memory size.

**Step 2: Contextual Attention Scores (if context provided)**
\[
S_{\text{context}} = \mathbf{c} \mathbf{C}^T \in \mathbb{R}^{b \times M}
\]
where \( \mathbf{C} \in \mathbb{R}^{M \times d_c} \) are the memory contexts.

**Step 3: Combined Content-Context Scores**
\[
S_{\text{combined}} = \begin{cases}
S_{\text{content}} & \text{if } \mathbf{c} = \text{None} \\
S_{\text{content}} + 0.5 \cdot S_{\text{context}} & \text{if } \mathbf{c} \neq \text{None}
\end{cases}
\]

**Step 4: Temporal Recency Bias**
\[
S_{\text{temporal}} = \exp(-\lambda \cdot (t_{\text{current}} - \mathbf{T})) \in \mathbb{R}^{M}
\]
where \( \mathbf{T} \in \mathbb{R}^{M \times 1} \) contains timestamps for each memory slot.

**Step 5: Final Attention Scores**
\[
S_{\text{final}} = S_{\text{combined}} + 0.3 \cdot \mathbf{1}_b \otimes S_{\text{temporal}}^T \in \mathbb{R}^{b \times M}
\]
where \( \mathbf{1}_b \) is a vector of ones with batch size \( b \), and \( \otimes \) denotes outer product.

**Step 6: Memory Slot Masking**
\[
S_{\text{masked}} = S_{\text{final}} \odot (\mathbf{1}_b \otimes \mathbf{m}^T) + (1 - \mathbf{1}_b \otimes \mathbf{m}^T) \odot (-10^9)
\]
where \( \mathbf{m} \in \{0,1\}^M \) is the used slots mask (1 for used, 0 for unused).

**Step 7: Softmax Attention Weights**
\[
\boldsymbol{\alpha} = \text{softmax}(S_{\text{masked}}) \in \mathbb{R}^{b \times M}
\]
\[
\alpha_{i,j} = \frac{\exp(S_{\text{masked}}[i,j])}{\sum_{k=1}^M \exp(S_{\text{masked}}[i,k])}
\]

**Step 8: Weighted Retrieval**
\[
\mathbf{r} = \boldsymbol{\alpha} \mathbf{V} \in \mathbb{R}^{b \times d}
\]
where \( \mathbf{V} \in \mathbb{R}^{M \times d} \) are the memory values.

### Mathematical Properties of the Forward Function

**1. Multi-Modal Fusion:**
The function combines three types of similarity:
- **Content similarity:** \( \cos(\mathbf{q}, \mathbf{k}_i) = \frac{\mathbf{q} \cdot \mathbf{k}_i}{\|\mathbf{q}\| \|\mathbf{k}_i\|} \)
- **Context similarity:** \( \cos(\mathbf{c}, \mathbf{c}_i) = \frac{\mathbf{c} \cdot \mathbf{c}_i}{\|\mathbf{c}\| \|\mathbf{c}_i\|} \)
- **Temporal similarity:** \( \exp(-\lambda \Delta t_i) \) where \( \Delta t_i = t_{\text{current}} - t_i \)

**2. Attention Weight Properties:**
\[
\sum_{j=1}^M \alpha_{i,j} = 1 \quad \forall i \in \{1, \ldots, b\}
\]
\[
\alpha_{i,j} \geq 0 \quad \forall i,j
\]

**3. Temporal Decay Function:**
The exponential decay ensures recent memories have higher influence:
\[
\lim_{\Delta t \rightarrow \infty} \exp(-\lambda \Delta t) = 0
\]
\[
\exp(-\lambda \cdot 0) = 1
\]

**4. Masking Effect:**
Unused memory slots are effectively removed from consideration:
\[
\lim_{x \rightarrow -\infty} \text{softmax}(x) = 0
\]

**5. Retrieval as Weighted Average:**
The final retrieval is a convex combination of memory values:
\[
\mathbf{r}_i = \sum_{j=1}^M \alpha_{i,j} \mathbf{v}_j
\]

### Computational Complexity

- **Time Complexity:** \( O(b \cdot M \cdot (d + d_c)) \) for attention computation
- **Space Complexity:** \( O(M \cdot (d + d_c + 1)) \) for memory storage

### Edge Cases

**1. No Used Slots:**
\[
\text{retrieve}(\mathbf{q}, \mathbf{c}, \lambda) = \mathcal{N}(0, \mathbf{I}_d) \quad \text{if } \sum_{i=1}^M m_i = 0
\]

**2. Single Used Slot:**
\[
\boldsymbol{\alpha} = [0, \ldots, 0, 1, 0, \ldots, 0] \quad \text{if only slot } k \text{ is used}
\]
\[
\mathbf{r} = \mathbf{v}_k
\]

**3. No Context Provided:**
\[
S_{\text{final}} = \mathbf{q} \mathbf{K}^T + 0.3 \cdot \mathbf{1}_b \otimes S_{\text{temporal}}^T
\]

This mathematical formulation shows how the forward function implements a sophisticated attention mechanism that balances content similarity, contextual relevance, and temporal recency to retrieve appropriate episodic memories.

---

