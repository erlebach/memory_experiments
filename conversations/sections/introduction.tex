\section{Introduction}
Memory is fundamental to both cognition and sequence modeling. While cognitive science has deve  loped sophisticated models of human episodic memory such as Context Maintenance and Retrieval (  CMR), the deep learning community has independently created powerful sequence models like RetNe  t and Mamba. Additionally, neuro-symbolic hybrid approaches like Titan have emerged that bridge   these domains. Despite their different origins and applications, these architectures share rem  arkably similar core dynamics.

In this work, we propose a unifying framework that reveals these diverse approaches as points i  n a broader space of state-based memory models. Our contributions include: (1) a general state-  space formulation that encompasses existing memory architectures, (2) a flexible implementation   framework supporting plug-and-play memory modules, and (3) comprehensive benchmarks across cog  nitive modeling, language modeling, and reasoning tasks.

